Script started on 2025-09-23 23:47:34-06:00 [TERM="dumb" TTY="/dev/pts/23" COLUMNS="121" LINES="36"]
ganesh@dknuth:~/repos/pytorch/issue-158003$ LD_PRELOAD=~/nixnan.so python run.py 
------------- NVBit (NVidia Binary Instrumentation Tool v1.7.5) Loaded --------------
NVBit core environment variables (mostly for nvbit-devs):
ACK_CTX_INIT_LIMITATION = 0 - if set, no warning will be printed for nvbit_at_ctx_init()
            NVDISASM = nvdisasm - override default nvdisasm found in PATH
            NOBANNER = 0 - if set, does not print this banner
       NO_EAGER_LOAD = 0 - eager module loading is turned on by NVBit to prevent potential NVBit tool deadlock, turn it off if you want to use the lazy module loading feature
---------------------------------------------------------------------------------
         INSTR_BEGIN = 0 - Beginning of the instruction interval where to apply instrumentation
           INSTR_END = 4294967295 - End of the instruction interval where to apply instrumentation
        TOOL_VERBOSE = 0 - Enable verbosity inside the tool
   ENABLE_FUN_DETAIL = 0 - Enable detailed function information for kernel
     PRINT_ILL_INSTR = 0 - Print the instruction which caused the exception
            SAMPLING = 0 - Instrument a repeat kernel every SAMPLING times
           INSTR_MEM = 0 - Instrument memory instructions for NaN/Inf detection
             LOGFILE =  - Path to the optional log file. Default is to print to stderr. This is useful for the case when an instrumented program is capturing stderr.
           LINE_INFO = 0 - Enable debug information for source code locations. This may cause crashes, so set this to 0 if you encounter issues.
#nixnan: ----------------------------------------------------------------------------------
WARNING: Do not call CUDA memory allocation in nvbit_at_ctx_init(). It will cause deadlocks. Do them in nvbit_tool_init(). If you encounter deadlocks, remove CUDA API calls to debug.
#nixnan: Initializing GPU context...
#nixnan: Could not open kernel_whitelist.txt!
#nixnan: Could not open kernel_blacklist.txt!
#nixnan: Instrumenting all kernels
#nixnan: running kernel [void at::native::tensor_kernel_scan_outer_dim] ...
#nixnan: running kernel [void at::native::vectorized_elementwise_kernel] ...
#nixnan: error [subnormal] detected in operand 2 of instruction FMUL R9, R9, c[0x0][0x168] ; in function void at::native::vectorized_elementwise_kernel at line 0 of type f32
#nixnan: error [subnormal] detected in operand 2 of instruction FMUL R8, R8, c[0x0][0x168] ; in function void at::native::vectorized_elementwise_kernel at line 0 of type f32
#nixnan: error [subnormal] detected in operand 2 of instruction FMUL R11, R11, c[0x0][0x168] ; in function void at::native::vectorized_elementwise_kernel at line 0 of type f32
#nixnan: error [subnormal] detected in operand 2 of instruction FMUL R10, R10, c[0x0][0x168] ; in function void at::native::vectorized_elementwise_kernel at line 0 of type f32
Traceback (most recent call last):
  File "/home/ganesh/repos/pytorch/issue-158003/run.py", line 47, in <module>
    print(torch.allclose(output, c_output, rtol=1e-3, atol=1e-3, equal_nan=True))
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 3.91 GiB. GPU 0 has a total capacity of 23.57 GiB of which 2.79 GiB is free. Including non-PyTorch memory, this process has 20.64 GiB memory in use. Of the allocated memory 16.63 GiB is allocated by PyTorch, and 2.94 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
#nixnan: Finalizing GPU context...

#nixnan: ------------ nixnan Report -----------

#nixnan: --- FP16 Operations ---
#nixnan: NaN:                    0 (0 repeats)
#nixnan: Infinity:               0 (0 repeats)
#nixnan: -Infinity:              0 (0 repeats)
#nixnan: Subnormal:              0 (0 repeats)
#nixnan: Division by 0:          0 (0 repeats)

#nixnan: --- BF16 Operations ---
#nixnan: NaN:                    0 (0 repeats)
#nixnan: Infinity:               0 (0 repeats)
#nixnan: -Infinity:              0 (0 repeats)
#nixnan: Subnormal:              0 (0 repeats)
#nixnan: Division by 0:          0 (0 repeats)

#nixnan: --- FP32 Operations ---
#nixnan: NaN:                    0 (0 repeats)
#nixnan: Infinity:               0 (0 repeats)
#nixnan: -Infinity:              0 (0 repeats)
#nixnan: Subnormal:              4 (524252 repeats)
#nixnan: Division by 0:          0 (0 repeats)

#nixnan: --- FP64 Operations ---
#nixnan: NaN:                    0 (0 repeats)
#nixnan: Infinity:               0 (0 repeats)
#nixnan: -Infinity:              0 (0 repeats)
#nixnan: Subnormal:              0 (0 repeats)
#nixnan: Division by 0:          0 (0 repeats)

#nixnan: --- FP16 Memory  Operations ---
#nixnan: NaN:                    0 (0 repeats)
#nixnan: --- BF16 Memory  Operations ---
#nixnan: NaN:                    0 (0 repeats)
#nixnan: --- FP32 Memory  Operations ---
#nixnan: NaN:                    0 (0 repeats)
#nixnan: --- FP64 Memory  Operations ---
#nixnan: NaN:                    0 (0 repeats)
ganesh@dknuth:~/repos/pytorch/issue-158003$ exit
exit

Script done on 2025-09-23 23:48:14-06:00 [COMMAND_EXIT_CODE="1"]
