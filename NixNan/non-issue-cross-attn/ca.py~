
class CrossAttention(nn.Module):
    def __init__(
            self, dim: int, nhead: int, dropout: float = 0.0, batch_first: bool = True,
            add_pe_to_qkv: List[bool] = [True, True, False], residual: bool = True, norm: bool = True
    ):
        super().__init__()
        self.cross_attn = nn.MultiheadAttention(dim, nhead, dropout=dropout, batch_first=batch_first)
        if norm:
            self.norm = nn.LayerNorm(dim)
        else:
            self.norm = nn.Identity()
        self.dropout = nn.Dropout(dropout)
        self.add_pe_to_qkv = add_pe_to_qkv
        self.residual = residual

    def forward(
            self, x: torch.Tensor, mem: torch.Tensor, x_pe: torch.Tensor, mem_pe: torch.Tensor, attn_mask: bool = None,
            *,
            need_weights: bool = False
    ) -> (torch.Tensor, torch.Tensor):
        x = self.norm(x)
        if self.add_pe_to_qkv[0]:
            q = x + x_pe
        else:
            q = x

        if any(self.add_pe_to_qkv[1:]):
            mem_with_pe = mem + mem_pe
            k = mem_with_pe if self.add_pe_to_qkv[1] else mem
            v = mem_with_pe if self.add_pe_to_qkv[2] else mem
        else:
            k = v = mem
        r = x
        x, weights = self.cross_attn(
            q, k, v, attn_mask=attn_mask, need_weights=need_weights, average_attn_weights=False
        )

        if self.residual:
            return r + self.dropout(x), weights
        else:
            return self.dropout(x), weights


