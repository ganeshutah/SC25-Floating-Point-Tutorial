Script started on 2025-09-23 23:32:07-06:00 [TERM="dumb" TTY="/dev/pts/23" COLUMNS="121" LINES="18"]
ganesh@dknuth:~/repos/pytorch/issue-156707$ LD_PRELOAD=~/nixnan.so python run.py
------------- NVBit (NVidia Binary Instrumentation Tool v1.7.5) Loaded --------------
NVBit core environment variables (mostly for nvbit-devs):
ACK_CTX_INIT_LIMITATION = 0 - if set, no warning will be printed for nvbit_at_ctx_init()
            NVDISASM = nvdisasm - override default nvdisasm found in PATH
            NOBANNER = 0 - if set, does not print this banner
       NO_EAGER_LOAD = 0 - eager module loading is turned on by NVBit to prevent potential NVBit tool deadlock, turn it off if you want to use the lazy module loading feature
---------------------------------------------------------------------------------
         INSTR_BEGIN = 0 - Beginning of the instruction interval where to apply instrumentation
           INSTR_END = 4294967295 - End of the instruction interval where to apply instrumentation
        TOOL_VERBOSE = 0 - Enable verbosity inside the tool
   ENABLE_FUN_DETAIL = 0 - Enable detailed function information for kernel
     PRINT_ILL_INSTR = 0 - Print the instruction which caused the exception
            SAMPLING = 0 - Instrument a repeat kernel every SAMPLING times
           INSTR_MEM = 0 - Instrument memory instructions for NaN/Inf detection
             LOGFILE =  - Path to the optional log file. Default is to print to stderr. This is useful for the case when an instrumented program is capturing stderr.
           LINE_INFO = 0 - Enable debug information for source code locations. This may cause crashes, so set this to 0 if you encounter issues.
#nixnan: ----------------------------------------------------------------------------------
CPU: [nan, nan, nan]
Traceback (most recent call last):
  File "/home/ganesh/repos/pytorch/issue-156707/run.py", line 15, in <module>
    out_mps = F.scaled_dot_product_attention(q.to('mps'), k.to('mps'), v.to('mps'), attn_mask=mask.to('mps'))
RuntimeError: PyTorch is not linked with support for mps devices
ganesh@dknuth:~/repos/pytorch/issue-156707$ ls
run.py	run-variant-cuda.py  run-variant-cuda.py~  scr.script  typescript  typescript-issue-156707
ganesh@dknuth:~/repos/pytorch/issue-156707$ LD_PRELOAD=~/nixnan.so python run-variant-cuda.py
------------- NVBit (NVidia Binary Instrumentation Tool v1.7.5) Loaded --------------
NVBit core environment variables (mostly for nvbit-devs):
ACK_CTX_INIT_LIMITATION = 0 - if set, no warning will be printed for nvbit_at_ctx_init()
            NVDISASM = nvdisasm - override default nvdisasm found in PATH
            NOBANNER = 0 - if set, does not print this banner
       NO_EAGER_LOAD = 0 - eager module loading is turned on by NVBit to prevent potential NVBit tool deadlock, turn it off if you want to use the lazy module loading feature
---------------------------------------------------------------------------------
         INSTR_BEGIN = 0 - Beginning of the instruction interval where to apply instrumentation
           INSTR_END = 4294967295 - End of the instruction interval where to apply instrumentation
        TOOL_VERBOSE = 0 - Enable verbosity inside the tool
   ENABLE_FUN_DETAIL = 0 - Enable detailed function information for kernel
     PRINT_ILL_INSTR = 0 - Print the instruction which caused the exception
            SAMPLING = 0 - Instrument a repeat kernel every SAMPLING times
           INSTR_MEM = 0 - Instrument memory instructions for NaN/Inf detection
             LOGFILE =  - Path to the optional log file. Default is to print to stderr. This is useful for the case when an instrumented program is capturing stderr.
           LINE_INFO = 0 - Enable debug information for source code locations. This may cause crashes, so set this to 0 if you encounter issues.
#nixnan: ----------------------------------------------------------------------------------
CPU: [nan, nan, nan]
WARNING: Do not call CUDA memory allocation in nvbit_at_ctx_init(). It will cause deadlocks. Do them in nvbit_tool_init(). If you encounter deadlocks, remove CUDA API calls to debug.
#nixnan: Initializing GPU context...
#nixnan: Could not open kernel_whitelist.txt!
#nixnan: Could not open kernel_blacklist.txt!
#nixnan: Instrumenting all kernels
#nixnan: running kernel [void at::native::vectorized_elementwise_kernel] ...
#nixnan: running kernel [void at::native::elementwise_kernel] ...
#nixnan: NaN immediate found in operand -QNAN
#nixnan: running kernel [fmha_cutlassF_f32_aligned_64x64_rf_sm80] ...
#nixnan: error [-infinity] detected in operand 2 of instruction @!P2 FADD R44, R44, R3 ; in function fmha_cutlassF_f32_aligned_64x64_rf_sm80 at line 0 of type f32
#nixnan: error [-infinity] detected in operand 0 of instruction @!P2 FADD R44, R44, R3 ; in function fmha_cutlassF_f32_aligned_64x64_rf_sm80 at line 0 of type f32
#nixnan: error [-infinity] detected in operand 2 of instruction @!P2 FADD R43, R43, R3 ; in function fmha_cutlassF_f32_aligned_64x64_rf_sm80 at line 0 of type f32
#nixnan: error [-infinity] detected in operand 0 of instruction @!P2 FADD R43, R43, R3 ; in function fmha_cutlassF_f32_aligned_64x64_rf_sm80 at line 0 of type f32
#nixnan: error [-infinity] detected in operand 1 of instruction FMUL R44, R44, 1.4426950216293334961 ; in function fmha_cutlassF_f32_aligned_64x64_rf_sm80 at line 0 of type f32
#nixnan: error [-infinity] detected in operand 0 of instruction FMUL R44, R44, 1.4426950216293334961 ; in function fmha_cutlassF_f32_aligned_64x64_rf_sm80 at line 0 of type f32
#nixnan: error [-infinity] detected in operand 1 of instruction FMUL R43, R43, 1.4426950216293334961 ; in function fmha_cutlassF_f32_aligned_64x64_rf_sm80 at line 0 of type f32
#nixnan: error [-infinity] detected in operand 0 of instruction FMUL R43, R43, 1.4426950216293334961 ; in function fmha_cutlassF_f32_aligned_64x64_rf_sm80 at line 0 of type f32
#nixnan: error [-infinity] detected in operand 1 of instruction FADD R0, R2, -R0 ; in function fmha_cutlassF_f32_aligned_64x64_rf_sm80 at line 0 of type f32
#nixnan: error [-infinity] detected in operand 0 of instruction FADD R0, R2, -R0 ; in function fmha_cutlassF_f32_aligned_64x64_rf_sm80 at line 0 of type f32
#nixnan: error [-infinity] detected in operand 1 of instruction @!P1 FMUL R0, R0, 0.5 ; in function fmha_cutlassF_f32_aligned_64x64_rf_sm80 at line 0 of type f32
#nixnan: error [-infinity] detected in operand 0 of instruction @!P1 FMUL R0, R0, 0.5 ; in function fmha_cutlassF_f32_aligned_64x64_rf_sm80 at line 0 of type f32
#nixnan: error [-infinity] detected in operand 1 of instruction MUFU.EX2 R0, R0 ; in function fmha_cutlassF_f32_aligned_64x64_rf_sm80 at line 0 of type f32
#nixnan: error [-infinity] detected in operand 1 of instruction @P0 FADD R4, R44, -R10 ; in function fmha_cutlassF_f32_aligned_64x64_rf_sm80 at line 0 of type f32
#nixnan: error [-infinity] detected in operand 0 of instruction @P0 FADD R4, R44, -R10 ; in function fmha_cutlassF_f32_aligned_64x64_rf_sm80 at line 0 of type f32
#nixnan: error [-infinity] detected in operand 1 of instruction @!P4 FMUL R4, R4, 0.5 ; in function fmha_cutlassF_f32_aligned_64x64_rf_sm80 at line 0 of type f32
#nixnan: error [-infinity] detected in operand 0 of instruction @!P4 FMUL R4, R4, 0.5 ; in function fmha_cutlassF_f32_aligned_64x64_rf_sm80 at line 0 of type f32
#nixnan: error [-infinity] detected in operand 1 of instruction @P0 MUFU.EX2 R4, R4 ; in function fmha_cutlassF_f32_aligned_64x64_rf_sm80 at line 0 of type f32
#nixnan: error [-infinity] detected in operand 1 of instruction @P6 FADD R4, R43, -R10 ; in function fmha_cutlassF_f32_aligned_64x64_rf_sm80 at line 0 of type f32
#nixnan: error [-infinity] detected in operand 0 of instruction @P6 FADD R4, R43, -R10 ; in function fmha_cutlassF_f32_aligned_64x64_rf_sm80 at line 0 of type f32
#nixnan: error [-infinity] detected in operand 1 of instruction @!P4 FMUL R4, R4, 0.5 ; in function fmha_cutlassF_f32_aligned_64x64_rf_sm80 at line 0 of type f32
#nixnan: error [-infinity] detected in operand 0 of instruction @!P4 FMUL R4, R4, 0.5 ; in function fmha_cutlassF_f32_aligned_64x64_rf_sm80 at line 0 of type f32
#nixnan: error [-infinity] detected in operand 1 of instruction @P6 MUFU.EX2 R4, R4 ; in function fmha_cutlassF_f32_aligned_64x64_rf_sm80 at line 0 of type f32
#nixnan: error [div0] detected in operand 1 of instruction @!P2 MUFU.RCP R2, R2 ; in function fmha_cutlassF_f32_aligned_64x64_rf_sm80 at line 0 of type f32
#nixnan: error [infinity] detected in operand 0 of instruction @!P2 MUFU.RCP R2, R2 ; in function fmha_cutlassF_f32_aligned_64x64_rf_sm80 at line 0 of type f32
#nixnan: error [infinity] detected in operand 2 of instruction FMUL R8, R4, R11.reuse ; in function fmha_cutlassF_f32_aligned_64x64_rf_sm80 at line 0 of type f32
#nixnan: error [NaN] detected in operand 0 of instruction FMUL R8, R4, R11.reuse ; in function fmha_cutlassF_f32_aligned_64x64_rf_sm80 at line 0 of type f32
#nixnan: error [infinity] detected in operand 2 of instruction FMUL R9, R5, R11.reuse ; in function fmha_cutlassF_f32_aligned_64x64_rf_sm80 at line 0 of type f32
#nixnan: error [NaN] detected in operand 0 of instruction FMUL R9, R5, R11.reuse ; in function fmha_cutlassF_f32_aligned_64x64_rf_sm80 at line 0 of type f32
#nixnan: error [infinity] detected in operand 2 of instruction FMUL R10, R6, R11.reuse ; in function fmha_cutlassF_f32_aligned_64x64_rf_sm80 at line 0 of type f32
#nixnan: error [NaN] detected in operand 0 of instruction FMUL R10, R6, R11.reuse ; in function fmha_cutlassF_f32_aligned_64x64_rf_sm80 at line 0 of type f32
#nixnan: error [infinity] detected in operand 2 of instruction FMUL R11, R7, R11 ; in function fmha_cutlassF_f32_aligned_64x64_rf_sm80 at line 0 of type f32
#nixnan: error [NaN] detected in operand 0 of instruction FMUL R11, R7, R11 ; in function fmha_cutlassF_f32_aligned_64x64_rf_sm80 at line 0 of type f32
CUDA: [nan, nan, nan]
#nixnan: Finalizing GPU context...

#nixnan: ------------ nixnan Report -----------

#nixnan: --- FP16 Operations ---
#nixnan: NaN:                    0 (0 repeats)
#nixnan: Infinity:               0 (0 repeats)
#nixnan: -Infinity:              0 (0 repeats)
#nixnan: Subnormal:              0 (0 repeats)
#nixnan: Division by 0:          0 (0 repeats)

#nixnan: --- BF16 Operations ---
#nixnan: NaN:                    0 (0 repeats)
#nixnan: Infinity:               0 (0 repeats)
#nixnan: -Infinity:              0 (0 repeats)
#nixnan: Subnormal:              0 (0 repeats)
#nixnan: Division by 0:          0 (0 repeats)

#nixnan: --- FP32 Operations ---
#nixnan: NaN:                    4 (0 repeats)
#nixnan: Infinity:               5 (0 repeats)
#nixnan: -Infinity:             23 (15 repeats)
#nixnan: Subnormal:              0 (0 repeats)
#nixnan: Division by 0:          1 (1 repeats)

#nixnan: --- FP64 Operations ---
#nixnan: NaN:                    0 (0 repeats)
#nixnan: Infinity:               0 (0 repeats)
#nixnan: -Infinity:              0 (0 repeats)
#nixnan: Subnormal:              0 (0 repeats)
#nixnan: Division by 0:          0 (0 repeats)

#nixnan: --- FP16 Memory  Operations ---
#nixnan: NaN:                    0 (0 repeats)
#nixnan: --- BF16 Memory  Operations ---
#nixnan: NaN:                    0 (0 repeats)
#nixnan: --- FP32 Memory  Operations ---
#nixnan: NaN:                    0 (0 repeats)
#nixnan: --- FP64 Memory  Operations ---
#nixnan: NaN:                    0 (0 repeats)
ganesh@dknuth:~/repos/pytorch/issue-156707$ ls
run.py	run-variant-cuda.py  run-variant-cuda.py~  scr.script  typescript  typescript-issue-156707
ganesh@dknuth:~/repos/pytorch/issue-156707$ exit
exit

Script done on 2025-09-23 23:33:24-06:00 [COMMAND_EXIT_CODE="0"]
